diff -uparN a/arch/arm64/include/asm/compat.h b/arch/arm64/include/asm/compat.h
--- a/arch/arm64/include/asm/compat.h	2019-02-14 20:20:12.339648609 +0800
+++ b/arch/arm64/include/asm/compat.h	2019-02-14 20:35:29.363672749 +0800
@@ -300,6 +300,15 @@ struct compat_shmid64_ds {
 
 static inline int is_compat_task(void)
 {
+#ifdef CONFIG_EXAGEAR_BT
+	return current_thread_info()->exagear_syscall || test_thread_flag(TIF_32BIT);
+#else
+	return test_thread_flag(TIF_32BIT);
+#endif
+}
+
+static inline int is_aarch32_compat_task(void)
+{
 	return test_thread_flag(TIF_32BIT);
 }
 
@@ -308,6 +317,13 @@ static inline int is_compat_thread(struc
 	return test_ti_thread_flag(thread, TIF_32BIT);
 }
 
+#ifdef CONFIG_EXAGEAR_BT
+static inline int is_exagear_compat_task(void)
+{
+	return current_thread_info()->exagear_syscall;
+}
+#endif
+
 #else /* !CONFIG_COMPAT */
 
 static inline int is_compat_thread(struct thread_info *thread)
diff -uparN a/arch/arm64/include/asm/mmu.h b/arch/arm64/include/asm/mmu.h
--- a/arch/arm64/include/asm/mmu.h	2019-02-14 20:20:12.343648609 +0800
+++ b/arch/arm64/include/asm/mmu.h	2019-02-14 20:35:29.363672749 +0800
@@ -27,6 +27,9 @@ typedef struct {
 	atomic64_t	id;
 	void		*vdso;
 	unsigned long	flags;
+#ifdef CONFIG_EXAGEAR_BT
+	unsigned long	exagear_mmap_base;
+#endif
 } mm_context_t;
 
 /*
diff -uparN a/arch/arm64/include/asm/pgtable.h b/arch/arm64/include/asm/pgtable.h
--- a/arch/arm64/include/asm/pgtable.h	2019-02-14 20:20:12.343648609 +0800
+++ b/arch/arm64/include/asm/pgtable.h	2019-02-14 20:35:29.363672749 +0800
@@ -737,6 +737,16 @@ static inline void update_mmu_cache(stru
 #define kc_vaddr_to_offset(v)	((v) & ~VA_START)
 #define kc_offset_to_vaddr(o)	((o) | VA_START)
 
+/*
+ * We provide our own arch_get_unmapped_area to handle 32-bit mmap calls from
+ * exagear.
+ */
+#ifdef CONFIG_EXAGEAR_BT
+#define HAVE_ARCH_UNMAPPED_AREA
+#define HAVE_ARCH_UNMAPPED_AREA_TOPDOWN
+#define HAVE_ARCH_HUGETLB_UNMAPPED_AREA
+#endif
+
 #endif /* !__ASSEMBLY__ */
 
 #endif /* __ASM_PGTABLE_H */
diff -uparN a/arch/arm64/include/asm/processor.h b/arch/arm64/include/asm/processor.h
--- a/arch/arm64/include/asm/processor.h	2019-02-14 20:20:12.343648609 +0800
+++ b/arch/arm64/include/asm/processor.h	2019-02-14 20:35:29.363672749 +0800
@@ -187,8 +187,31 @@ extern struct task_struct *cpu_switch_to
 #define task_pt_regs(p) \
 	((struct pt_regs *)(THREAD_SIZE + task_stack_page(p)) - 1)
 
+#ifdef CONFIG_EXAGEAR_BT
+
+#define KSTK_EIP(tsk)							\
+({									\
+	unsigned long __out;						\
+	if (task_thread_info(tsk)->exagear_syscall)			\
+		__out = (unsigned long)task_pt_regs(tsk)->regs[15];	\
+	else								\
+		__out = (unsigned long)task_pt_regs(tsk)->pc;		\
+	__out;								\
+ })
+#define KSTK_ESP(tsk)							\
+({									\
+	unsigned long __out;						\
+	if (task_thread_info(tsk)->exagear_syscall)			\
+		__out = (unsigned long)task_pt_regs(tsk)->regs[13];	\
+	else								\
+		__out = user_stack_pointer(task_pt_regs(tsk));		\
+	__out;								\
+ })
+
+#else
 #define KSTK_EIP(tsk)	((unsigned long)task_pt_regs(tsk)->pc)
 #define KSTK_ESP(tsk)	user_stack_pointer(task_pt_regs(tsk))
+#endif
 
 /*
  * Prefetching support
diff -uparN a/arch/arm64/include/asm/thread_info.h b/arch/arm64/include/asm/thread_info.h
--- a/arch/arm64/include/asm/thread_info.h	2019-02-14 20:20:12.343648609 +0800
+++ b/arch/arm64/include/asm/thread_info.h	2019-02-14 20:35:29.363672749 +0800
@@ -43,6 +43,9 @@ struct thread_info {
 	u64			ttbr0;		/* saved TTBR0_EL1 */
 #endif
 	int			preempt_count;	/* 0 => preemptable, <0 => bug */
+#ifdef CONFIG_EXAGEAR_BT
+	int			exagear_syscall;	/* exagear 32-bit syscall */
+#endif
 };
 
 #define INIT_THREAD_INFO(tsk)						\
diff -uparN a/arch/arm64/Kconfig b/arch/arm64/Kconfig
--- a/arch/arm64/Kconfig	2019-02-14 20:20:12.347648609 +0800
+++ b/arch/arm64/Kconfig	2019-02-14 20:35:29.359672749 +0800
@@ -849,6 +849,16 @@ config XEN
 	help
 	  Say Y if you want to run Linux in a Virtual Machine on Xen on ARM64.
 
+config EXAGEAR_BT
+	bool "Exagear binary translator support"
+	depends on COMPAT
+	select CHECKPOINT_RESTORE
+	select BINFMT_MISC
+	default y
+	help
+	  Kernel support for the Exagear binary translator which dynamically
+	  translates 32-bit ARM binaries into 64-bit code.
+
 config FORCE_MAX_ZONEORDER
 	int
 	default "14" if (ARM64_64K_PAGES && TRANSPARENT_HUGEPAGE)
diff -uparN a/arch/arm64/kernel/asm-offsets.c b/arch/arm64/kernel/asm-offsets.c
--- a/arch/arm64/kernel/asm-offsets.c	2019-02-14 20:20:12.343648609 +0800
+++ b/arch/arm64/kernel/asm-offsets.c	2019-02-14 20:35:29.363672749 +0800
@@ -45,6 +45,9 @@ int main(void)
   DEFINE(TSK_TI_TTBR0,		offsetof(struct task_struct, thread_info.ttbr0));
 #endif
   DEFINE(TSK_STACK,		offsetof(struct task_struct, stack));
+#ifdef CONFIG_EXAGEAR_BT
+  DEFINE(TI_EXAGEAR_SYSCALL,	offsetof(struct task_struct, thread_info.exagear_syscall));
+#endif
   BLANK();
   DEFINE(THREAD_CPU_CONTEXT,	offsetof(struct task_struct, thread.cpu_context));
   BLANK();
diff -uparN a/arch/arm64/kernel/entry.S b/arch/arm64/kernel/entry.S
--- a/arch/arm64/kernel/entry.S	2019-02-14 20:20:12.343648609 +0800
+++ b/arch/arm64/kernel/entry.S	2019-02-15 11:45:13.259997581 +0800
@@ -715,6 +715,11 @@ el0_sync_compat:
 	cmp	x24, #ESR_ELx_EC_BREAKPT_LOW	// debug exception in EL0
 	b.ge	el0_dbg
 	b	el0_inv
+#ifdef CONFIG_EXAGEAR_BT
+el0_svc_exagear:
+	mov w27, #1
+	str w27, [tsk, #TI_EXAGEAR_SYSCALL]	// set exagear syscall flag
+#endif
 el0_svc_compat:
 	/*
 	 * AArch32 syscall handling
@@ -899,6 +904,9 @@ ENDPROC(el0_error)
  */
 ret_fast_syscall:
 	disable_daif
+	#ifdef CONFIG_EXAGEAR_BT
+	str	wzr, [tsk, #TI_EXAGEAR_SYSCALL]	// clear exagear syscall flag
+	#endif
 	str	x0, [sp, #S_X0]			// returned x0
 	ldr	x1, [tsk, #TSK_TI_FLAGS]	// re-check for syscall tracing
 	and	x2, x1, #_TIF_SYSCALL_WORK
@@ -940,6 +948,9 @@ ENDPROC(ret_to_user)
  */
 	.align	6
 el0_svc:
+#ifdef CONFIG_EXAGEAR_BT
+	tbnz	w8, #31, el0_svc_exagear		// redirect to compat syscall handler
+#endif
 	ldr	x16, [tsk, #TSK_TI_FLAGS]	// load thread flags
 	adrp	stbl, sys_call_table		// load syscall table pointer
 	mov	wscno, w8			// syscall number in w8
@@ -1013,6 +1024,9 @@ __sys_trace_return:
 __sys_trace_return_skipped:
 	mov	x0, sp
 	bl	syscall_trace_exit
+#ifdef CONFIG_EXAGEAR_BT
+	str	wzr, [tsk, #TI_EXAGEAR_SYSCALL]	// clear exagear syscall flag
+#endif
 	b	ret_to_user
 
 __ni_sys_trace:
diff -uparN a/arch/arm64/kernel/fpsimd.c b/arch/arm64/kernel/fpsimd.c
--- a/arch/arm64/kernel/fpsimd.c	2019-02-14 20:20:12.343648609 +0800
+++ b/arch/arm64/kernel/fpsimd.c	2019-02-15 11:32:48.212027474 +0800
@@ -830,7 +830,7 @@ void fpsimd_release_task(struct task_str
 asmlinkage void do_sve_acc(unsigned int esr, struct pt_regs *regs)
 {
 	/* Even if we chose not to use SVE, the hardware could still trap: */
-	if (unlikely(!system_supports_sve()) || WARN_ON(is_compat_task())) {
+	if (unlikely(!system_supports_sve()) || WARN_ON(is_aarch32_compat_task())) {
 		force_signal_inject(SIGILL, ILL_ILLOPC, regs, 0);
 		return;
 	}
diff -uparN a/arch/arm64/kernel/process.c b/arch/arm64/kernel/process.c
--- a/arch/arm64/kernel/process.c	2019-02-14 20:20:12.343648609 +0800
+++ b/arch/arm64/kernel/process.c	2019-02-14 20:35:29.363672749 +0800
@@ -257,7 +257,7 @@ static void tls_thread_flush(void)
 {
 	write_sysreg(0, tpidr_el0);
 
-	if (is_compat_task()) {
+	if (is_aarch32_compat_task()) {
 		current->thread.tp_value = 0;
 
 		/*
@@ -481,7 +481,7 @@ unsigned long arch_align_stack(unsigned
 
 unsigned long arch_randomize_brk(struct mm_struct *mm)
 {
-	if (is_compat_task())
+	if (is_aarch32_compat_task())
 		return randomize_page(mm->brk, SZ_32M);
 	else
 		return randomize_page(mm->brk, SZ_1G);
@@ -492,5 +492,8 @@ unsigned long arch_randomize_brk(struct
  */
 void arch_setup_new_exec(void)
 {
-	current->mm->context.flags = is_compat_task() ? MMCF_AARCH32 : 0;
+#ifdef CONFIG_EXAGEAR_BT
+	current_thread_info()->exagear_syscall = 0;
+#endif
+	current->mm->context.flags = is_aarch32_compat_task() ? MMCF_AARCH32 : 0;
 }
diff -uparN a/arch/arm64/kernel/ptrace.c b/arch/arm64/kernel/ptrace.c
--- a/arch/arm64/kernel/ptrace.c	2019-02-14 20:20:12.343648609 +0800
+++ b/arch/arm64/kernel/ptrace.c	2019-02-14 20:35:29.363672749 +0800
@@ -191,7 +191,7 @@ static void ptrace_hbptriggered(struct p
 #ifdef CONFIG_COMPAT
 	int i;
 
-	if (!is_compat_task())
+	if (!is_aarch32_compat_task())
 		goto send_sig;
 
 	for (i = 0; i < ARM_MAX_BRP; ++i) {
diff -uparN a/arch/arm64/kernel/signal.c b/arch/arm64/kernel/signal.c
--- a/arch/arm64/kernel/signal.c	2019-02-14 20:20:12.343648609 +0800
+++ b/arch/arm64/kernel/signal.c	2019-02-14 20:35:29.363672749 +0800
@@ -794,7 +794,7 @@ static void handle_signal(struct ksignal
 	/*
 	 * Set up the stack frame
 	 */
-	if (is_compat_task()) {
+	if (is_aarch32_compat_task()) {
 		if (ksig->ka.sa.sa_flags & SA_SIGINFO)
 			ret = compat_setup_rt_frame(usig, ksig, oldset, regs);
 		else
diff -uparN a/arch/arm64/kernel/traps.c b/arch/arm64/kernel/traps.c
--- a/arch/arm64/kernel/traps.c	2019-02-14 20:20:12.347648609 +0800
+++ b/arch/arm64/kernel/traps.c	2019-02-14 20:35:29.363672749 +0800
@@ -520,7 +520,7 @@ asmlinkage long do_ni_syscall(struct pt_
 {
 #ifdef CONFIG_COMPAT
 	long ret;
-	if (is_compat_task()) {
+	if (is_aarch32_compat_task()) {
 		ret = compat_arm_syscall(regs);
 		if (ret != -ENOSYS)
 			return ret;
diff -uparN a/arch/arm64/mm/mmap.c b/arch/arm64/mm/mmap.c
--- a/arch/arm64/mm/mmap.c	2019-02-14 20:20:12.347648609 +0800
+++ b/arch/arm64/mm/mmap.c	2019-02-14 20:35:29.363672749 +0800
@@ -28,6 +28,8 @@
 #include <linux/io.h>
 #include <linux/personality.h>
 #include <linux/random.h>
+#include <linux/security.h>
+#include <linux/hugetlb.h>
 
 #include <asm/cputype.h>
 
@@ -38,6 +40,13 @@
 #define MIN_GAP (SZ_128M)
 #define MAX_GAP	(STACK_TOP/6*5)
 
+#ifdef CONFIG_EXAGEAR_BT
+/* Definitions for Exagear's guest mmap area */
+#define EXAGEAR_STACK_TOP			0xffff0000
+#define EXAGEAR_MAX_GAP			(EXAGEAR_STACK_TOP/6*5)
+#define EXAGEAR_TASK_UNMAPPED_BASE	PAGE_ALIGN(TASK_SIZE_32 / 4)
+#endif
+
 static int mmap_is_legacy(void)
 {
 	if (current->personality & ADDR_COMPAT_LAYOUT)
@@ -79,6 +88,20 @@ static unsigned long mmap_base(unsigned
 	return PAGE_ALIGN(STACK_TOP - gap - rnd);
 }
 
+#ifdef CONFIG_EXAGEAR_BT
+static unsigned long exagear_mmap_base(unsigned long rnd)
+{
+	unsigned long gap = rlimit(RLIMIT_STACK);
+
+	if (gap < MIN_GAP)
+		gap = MIN_GAP;
+	else if (gap > EXAGEAR_MAX_GAP)
+		gap = EXAGEAR_MAX_GAP;
+
+	return PAGE_ALIGN(EXAGEAR_STACK_TOP - gap - rnd);
+}
+#endif
+
 /*
  * This function, called very early during the creation of a new process VM
  * image, sets up which VM layout function to use:
@@ -86,9 +109,18 @@ static unsigned long mmap_base(unsigned
 void arch_pick_mmap_layout(struct mm_struct *mm)
 {
 	unsigned long random_factor = 0UL;
+#ifdef CONFIG_EXAGEAR_BT
+	unsigned long exagear_random_factor = 0UL;
+#endif
 
-	if (current->flags & PF_RANDOMIZE)
+	if (current->flags & PF_RANDOMIZE) {
 		random_factor = arch_mmap_rnd();
+#ifdef CONFIG_EXAGEAR_BT
+		exagear_random_factor = (get_random_long() &
+			((1UL << mmap_rnd_compat_bits) - 1)) << PAGE_SHIFT;
+#endif
+
+	}
 
 	/*
 	 * Fall back to the standard layout if the personality bit is set, or
@@ -97,9 +129,16 @@ void arch_pick_mmap_layout(struct mm_str
 	if (mmap_is_legacy()) {
 		mm->mmap_base = TASK_UNMAPPED_BASE + random_factor;
 		mm->get_unmapped_area = arch_get_unmapped_area;
+#ifdef CONFIG_EXAGEAR_BT
+		mm->context.exagear_mmap_base = EXAGEAR_TASK_UNMAPPED_BASE +
+			exagear_random_factor;
+#endif
 	} else {
 		mm->mmap_base = mmap_base(random_factor);
 		mm->get_unmapped_area = arch_get_unmapped_area_topdown;
+#ifdef CONFIG_EXAGEAR_BT
+		mm->context.exagear_mmap_base = exagear_mmap_base(exagear_random_factor);
+#endif
 	}
 }
 
@@ -151,3 +190,183 @@ int devmem_is_allowed(unsigned long pfn)
 }
 
 #endif
+
+#ifdef CONFIG_EXAGEAR_BT
+
+/* Get an address range which is currently unmapped.
+ * For shmat() with addr=0.
+ *
+ * Ugly calling convention alert:
+ * Return value with the low bits set means error value,
+ * ie
+ *	if (ret & ~PAGE_MASK)
+ *		error = ret;
+ *
+ * This function "knows" that -ENOMEM has the bits set.
+ */
+unsigned long
+arch_get_unmapped_area(struct file *filp, unsigned long addr,
+		unsigned long len, unsigned long pgoff, unsigned long flags)
+{
+	struct mm_struct *mm = current->mm;
+	struct vm_area_struct *vma, *prev;
+	struct vm_unmapped_area_info info;
+	bool bad_addr = false;
+
+	if (len > TASK_SIZE - mmap_min_addr)
+		return -ENOMEM;
+
+	/*
+	 * Ensure that translated processes do not allocate the last
+	 * page of the 32-bit address space, or anything above it.
+	 */
+	if (is_exagear_compat_task())
+		bad_addr = addr + len > TASK_SIZE_32 - PAGE_SIZE;
+
+	if (flags & MAP_FIXED)
+		return bad_addr ? -ENOMEM : addr;
+
+	if (addr && !bad_addr) {
+		addr = PAGE_ALIGN(addr);
+		vma = find_vma_prev(mm, addr, &prev);
+		if (TASK_SIZE - len >= addr && addr >= mmap_min_addr &&
+		    (!vma || addr + len <= vm_start_gap(vma)) &&
+		    (!prev || addr >= vm_end_gap(prev)))
+			return addr;
+	}
+
+	info.flags = 0;
+	info.length = len;
+	if (is_exagear_compat_task()) {
+		info.low_limit = mm->context.exagear_mmap_base;
+		info.high_limit = TASK_SIZE_32 - PAGE_SIZE;
+	} else {
+		info.low_limit = mm->mmap_base;
+		info.high_limit = TASK_SIZE;
+	}
+	info.align_mask = 0;
+	return vm_unmapped_area(&info);
+}
+
+/*
+ * This mmap-allocator allocates new areas top-down from below the
+ * stack's low limit (the base):
+ */
+unsigned long
+arch_get_unmapped_area_topdown(struct file *filp, const unsigned long addr0,
+			  const unsigned long len, const unsigned long pgoff,
+			  const unsigned long flags)
+{
+	struct vm_area_struct *vma, *prev;
+	struct mm_struct *mm = current->mm;
+	unsigned long addr = addr0;
+	struct vm_unmapped_area_info info;
+	bool bad_addr = false;
+
+	/* requested length too big for entire address space */
+	if (len > TASK_SIZE - mmap_min_addr)
+		return -ENOMEM;
+
+	/*
+	 * Ensure that translated processes do not allocate the last
+	 * page of the 32-bit address space, or anything above it.
+	 */
+	if (is_exagear_compat_task())
+		bad_addr = addr + len > TASK_SIZE_32 - PAGE_SIZE;
+
+	if (flags & MAP_FIXED)
+		return bad_addr ? -ENOMEM : addr;
+
+	/* requesting a specific address */
+	if (addr && !bad_addr) {
+		addr = PAGE_ALIGN(addr);
+		vma = find_vma_prev(mm, addr, &prev);
+		if (TASK_SIZE - len >= addr && addr >= mmap_min_addr &&
+				(!vma || addr + len <= vm_start_gap(vma)) &&
+				(!prev || addr >= vm_end_gap(prev)))
+			return addr;
+	}
+
+	info.flags = VM_UNMAPPED_AREA_TOPDOWN;
+	info.length = len;
+	info.low_limit = max(PAGE_SIZE, mmap_min_addr);
+	if (is_exagear_compat_task())
+		info.high_limit = mm->context.exagear_mmap_base;
+	else
+		info.high_limit = mm->mmap_base;
+	info.align_mask = 0;
+	addr = vm_unmapped_area(&info);
+
+	/*
+	 * A failed mmap() very likely causes application failure,
+	 * so fall back to the bottom-up function here. This scenario
+	 * can happen with large stack limits and large mmap()
+	 * allocations.
+	 */
+	if (offset_in_page(addr)) {
+		VM_BUG_ON(addr != -ENOMEM);
+		info.flags = 0;
+		if (is_exagear_compat_task()) {
+			info.low_limit = EXAGEAR_TASK_UNMAPPED_BASE;
+			info.high_limit = TASK_SIZE_32 - PAGE_SIZE;
+		} else {
+			info.low_limit = TASK_UNMAPPED_BASE;
+			info.high_limit = TASK_SIZE;
+		}
+		addr = vm_unmapped_area(&info);
+	}
+
+	return addr;
+}
+
+unsigned long
+hugetlb_get_unmapped_area(struct file *file, unsigned long addr,
+		unsigned long len, unsigned long pgoff, unsigned long flags)
+{
+	struct mm_struct *mm = current->mm;
+	struct vm_area_struct *vma;
+	struct hstate *h = hstate_file(file);
+	struct vm_unmapped_area_info info;
+	bool bad_addr = false;
+
+	if (len & ~huge_page_mask(h))
+		return -EINVAL;
+	if (len > TASK_SIZE)
+		return -ENOMEM;
+
+	/*
+	 * Ensure that translated processes do not allocate the last
+	 * page of the 32-bit address space, or anything above it.
+	 */
+	if (is_exagear_compat_task())
+		bad_addr = addr + len > TASK_SIZE_32 - PAGE_SIZE;
+
+	if (flags & MAP_FIXED) {
+		if (prepare_hugepage_range(file, addr, len))
+			return -EINVAL;
+		return bad_addr ? -ENOMEM : addr;
+	}
+
+	if (addr && !bad_addr) {
+		addr = ALIGN(addr, huge_page_size(h));
+		vma = find_vma(mm, addr);
+		if (TASK_SIZE - len >= addr &&
+		    (!vma || addr + len <= vm_start_gap(vma)))
+			return addr;
+	}
+
+	info.flags = 0;
+	info.length = len;
+	if (is_exagear_compat_task()) {
+		info.low_limit = EXAGEAR_TASK_UNMAPPED_BASE;
+		info.high_limit = TASK_SIZE_32 - PAGE_SIZE;
+	} else {
+		info.low_limit = TASK_UNMAPPED_BASE;
+		info.high_limit = TASK_SIZE;
+	}
+	info.align_mask = PAGE_MASK & ~huge_page_mask(h);
+	info.align_offset = 0;
+	return vm_unmapped_area(&info);
+}
+
+#endif
